require(caret)  # contains most of the prediction functions we'll use
install.packages("rpart")
install.packages("rpart")
install.packages("randomForest")
install.packages("e1071")
library(caret)  # contains most of the prediction functions we'll use
library(caret)  # contains most of the prediction functions we'll use
library(rpart)  # needed for the plotting
library(randomForest)
library(e1071)
# read in training set
train <- read.csv("../../../data/machine-learning/mann-whitney/model-8020.csv")
# read in training set
train <- read.csv("../../../data/machine-learning/mann-whitney/model-8020.csv")
train_class <- train[,4]
train_exp <- train[,6:387]
endo_train <- cbind(train_class, train_exp)
colnames(endo_train)[1] <- "severity"
# read in test set
test <- read.csv("../../../data/machine-learning/mann-whitney/validation-8020.csv")
test_class <- test[,4]
test_exp <- test[,6:387]
test_exp <- test[,6:387]
endo_test <- cbind(test_class, test_exp)
colnames(endo_test)[1] <- "severity"
# define tuning parameters
fitControl <- trainControl(method = "cv", number = 5)
values.k <- data.frame(k = 1:100)
# run kNN model with 5-fold CV and various k's
set.seed(4747)
trainKNN <- train(severity ~ ., data = endo_train,
method = "knn", trControl = fitControl,
tuneGrid = values.k)
trainKNN <- train(severity ~ ., data = endo_train,
method = "knn", trControl = fitControl,
tuneGrid = values.k)
# output results to find optimal k
trainKNN$finalModel
# define tuning parameters
fitControl <- trainControl(method = "cv", number = 5)
values.k <- data.frame(k = 1:95)
# run kNN model with 5-fold CV and various k's
set.seed(4747)
trainKNN <- train(severity ~ ., data = endo_train,
method = "knn", trControl = fitControl,
tuneGrid = values.k)
# output results to find optimal k
trainKNN$finalModel
# define tuning parameters
fitControl <- trainControl(method = "cv", number = 5)
values.k <- data.frame(k = 1:94)
# run kNN model with 5-fold CV and various k's
set.seed(4747)
trainKNN <- train(severity ~ ., data = endo_train,
method = "knn", trControl = fitControl,
tuneGrid = values.k)
# output results to find optimal k
trainKNN$finalModel
trainKNN
# output results to find optimal k
trainKNN$finalModel
# obtain accuracy on the OUTER test set with k = 51 model
set.seed(47)
predict_knn <- confusionMatrix(data = predict(trainKNN, newdata = endo_test),
reference = endo_test$severity)
# print test accuracy
knn_accuracy <- predict_knn$overall[1]
knn_accuracy
# set tuning method and parameter
control <- trainControl(method = "oob")
mtry.tune <- data.frame(mtry = 1:100)
# run and optimize random forest with ntree = 250
# optimal mtry = 6
# OOB error = 28.57%
set.seed(4747)
rf.250 <- train(severity ~ ., data = endo_train, method = "rf",
trControl = control, na.action = na.roughfix,
tuneGrid = mtry.tune, ntree = 250, importance = TRUE)
rf.250 <- train(severity ~ ., data = endo_train, method = "rf",
trControl = control, na.action = na.roughfix,
tuneGrid = mtry.tune, ntree = 250, importance = TRUE)
rf.250$finalModel
rf.250$finalModel
# set tuning method and parameter
control <- trainControl(method = "oob"
mtry.tune <- data.frame(mtry = 1:100
# set tuning method and parameter
control <- trainControl(method = "oob")
mtry.tune <- data.frame(mtry = 1:100)
rf.250
rf.250$finalModel
rf.300$finalModel
# run with ntree = 300
# optimal mtry = 10
# OOB error = 27.82 %
set.seed(4747)
rf.300 <- train(severity ~ ., data = endo_train, method = "rf",
trControl = control, na.action = na.roughfix,
tuneGrid = mtry.tune, ntree = 300, importance = TRUE)
rf.300$finalModel
# run with ntree = 400
# optimal mtry = 17
# OOB error = 27.07
set.seed(4747)
rf.400 <- train(severity ~ ., data = endo_train, method = "rf",
trControl = control, na.action = na.roughfix,
tuneGrid = mtry.tune, ntree = 400, importance = TRUE)
rf.400$finalModel
# run with ntree = 500
# optimal mtry = 17
# OOB error = 27.82 %
set.seed(4747)
rf.500 <- train(severity ~ ., data = endo_train, method = "rf",
trControl = control, na.action = na.roughfix,
tuneGrid = mtry.tune, ntree = 500, importance = TRUE)
rf.500$finalModel
# run with ntree = 550
# optimal mtry = 17
# OOB error = 29.32 %
set.seed(4747)
rf.550 <- train(severity ~ ., data = endo_train, method = "rf",
trControl = control, na.action = na.roughfix,
tuneGrid = mtry.tune, ntree = 550, importance = TRUE)
rf.550$finalModel
confusionMatrix
set.seed(47)
predict_rf <- confusionMatrix(data = predict(rf.final, newdata = endo_test),
reference = endo_test$severity)
# build the final model with mtry = 17 and ntree = 400
rf.final <- rf.400
rf.final
rf.600$finalModel
# run with ntree = 600
# optimal mtry = 8
# OOB error = 30.08 %
set.seed(4747)
rf.600 <- train(severity ~ ., data = endo_train, method = "rf",
trControl = control, na.action = na.roughfix,
tuneGrid = mtry.tune, ntree = 600, importance = TRUE)
rf.600$finalModel
# run with ntree = 900
# optimal mtry = 17
# OOB error = 27.82 %
set.seed(4747)
rf.900 <- train(severity ~ ., data = endo_train, method = "rf",
trControl = control, na.action = na.roughfix,
tuneGrid = mtry.tune, ntree = 900, importance = TRUE)
rf.900$finalModel
# run with ntree = 1000
# optimal mtry = 17
# OOB error = 28.57 %
set.seed(4747)
rf.1000 <- train(severity ~ ., data = endo_train, method = "rf",
trControl = control, na.action = na.roughfix,
tuneGrid = mtry.tune, ntree = 1000, importance=TRUE)
rf.1000$finalModel
rf.700$finalModel
# run with ntree = 700
# optimal mtry = 17
# OOB error = 29.32 %
set.seed(4747)
rf.700 <- train(severity ~ ., data = endo_train, method = "rf",
trControl = control, na.action = na.roughfix,
tuneGrid = mtry.tune, ntree = 700, importance = TRUE)
rf.700$finalModel
# run with ntree = 800
# optimal mtry = 17
# OOB error = 27.82 %
set.seed(4747)
rf.800 <- train(severity ~ ., data = endo_train, method = "rf",
trControl = control, na.action = na.roughfix,
tuneGrid = mtry.tune, ntree = 800, importance = TRUE)
rf.800$finalModel
# build the final model with mtry = 17 and ntree = 400
rf.final <- rf.250
# build the final model with mtry = 17 and ntree = 400
rf.final <- rf.250
# try using Random Forest function
set.seed(47)
rf.function <- randomForest(severity ~ ., data = endo_train, mtry = 17, ntree = 400, importance = TRUE)
rf.function
set.seed(47)
predict_rf <- confusionMatrix(data = predict(rf.final, newdata = endo_test),
reference = endo_test$severity)
predict_rf$table
# print test accuracy
rf_accuracy <- predict_rf$overall[1]
rf_accuracy
set.seed(47)
# create vector for costs
cost <- c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)
# fit linear support vector classifier
svmL <- train(severity ~ ., data = endo_train, method="svmLinear",
trControl = trainControl(method = "cv", number = 5),
tuneGrid = expand.grid(C = cost),
preProcess = c("center", "scale"))
# output the results
svmL
svmL$finalModel
predict_svm <- confusionMatrix(data = predict(svmL, newdata = endo_test),
reference = endo_test$severity)
predict_svm$table
# print test accuracy
svm_accuracy <- predict_svm$overall[1]
svm_accuracy
# ANOTHER WAY
# predict the test data
predict_SVM <- predict(svmL, endo_test)
confusion_matrix <- table(endo_test$severity, predict_SVM)
# calculate test error rate
linearError <-  (confusion_matrix[1,2] + confusion_matrix[2,1]) / sum(confusion_matrix)
linearError
cost <- c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)
ten_fold <- sapply(1:100, function(t) {
random_ix <- sample(x=c(1:148), size=round(148*0.8), replace=FALSE)
random_ix_2 <- setdiff(c(1:148), random_ix)
train_core <- core_genes[random_ix,]
test_core <- core_genes[-random_ix,]
trainSVM <- train(endo ~ ., data = train_core, method = "svmLinear",
trControl = trainControl(method = "none"),
tuneGrid = expand.grid(C = 0.10),
preProcess = c("center", "scale"))
test.predict <- predict(trainSVM, test_core)
confusion_matrix <- table(test_core$endo, test.predict)
linearError <-  (confusion_matrix[1,2] + confusion_matrix[2,1]) / sum(confusion_matrix)
linearError
})
core_genes
core_data <- read.csv("../../data/genbank/core_data.csv")
# # read in the core gene data
core_data <- read.csv("../../../data/genbank/06-twofold-genes.csv")
core_class <- core_data[,4] # obtain endo classifications
core_exp <- core_data[,6:387] # expression data
core_genes <- cbind(core_class, core_exp)
colnames(core_genes)[1] <- "endo"
ten_fold <- sapply(1:100, function(t) {
random_ix <- sample(x=c(1:148), size=round(148*0.8), replace=FALSE)
random_ix_2 <- setdiff(c(1:148), random_ix)
train_core <- core_genes[random_ix,]
test_core <- core_genes[-random_ix,]
trainSVM <- train(endo ~ ., data = train_core, method = "svmLinear",
trControl = trainControl(method = "none"),
tuneGrid = expand.grid(C = 0.10),
preProcess = c("center", "scale"))
test.predict <- predict(trainSVM, test_core)
confusion_matrix <- table(test_core$endo, test.predict)
linearError <-  (confusion_matrix[1,2] + confusion_matrix[2,1]) / sum(confusion_matrix)
linearError
})
ten_fold <- sapply(1:100, function(t) {
random_ix <- sample(x=c(1:148), size=round(148*0.8), replace=FALSE)
random_ix_2 <- setdiff(c(1:148), random_ix)
train_core <- core_genes[random_ix,]
test_core <- core_genes[-random_ix,]
trainSVM <- train(endo ~ ., data = train_core, method = "svmLinear",
trControl = trainControl(method = "none"),
tuneGrid = expand.grid(C = 0.10),
preProcess = c("center", "scale"))
test.predict <- predict(trainSVM, test_core)
confusion_matrix <- table(test_core$endo, test.predict)
linearError <-  (confusion_matrix[1,2] + confusion_matrix[2,1]) / sum(confusion_matrix)
linearError
})
mean(ten_fold)
